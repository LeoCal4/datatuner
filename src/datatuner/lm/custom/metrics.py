from typing import List, Tuple, Dict

import sacrebleu 
from rouge_metric import PyRouge
# from moverscore_v2 import get_idf_dict, word_mover_score
import nltk 
from nltk.translate.meteor_score import meteor_score


def group_inputs_and_outputs_by_data(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> Tuple[List[List[str]], List[str]]:
    """Aggregate all the sentences which share the same original data.
    This is a slightly simplified data collection procedure of bleu(...) found in lm/metrics.py

    Args:
        inputs_and_outputs (List[Tuple[str, str, List[str]]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences 

    Returns:
        Tuple(List[List[str]], List[str]): all the originals sentences and the predictions, grouped by data
    """
    grouped_items = {}
    for data, original_sentence, generated_sentences in model_inputs_and_outputs:
        generated_sentence = generated_sentences[0]
        #* lowercase sentences
        generated_sentence = generated_sentence.lower()
        original_sentence = original_sentence.lower()
        #* group the og and gen sentences by the values of their other keys (basically their data)
        if data in grouped_items:
            grouped_items[data]["original"].append(original_sentence)
            # this considerings just the last found sentence generated by certain data since the outputs doesn't change 
            grouped_items[data]["prediction"] = generated_sentence
        else:
            grouped_items[data] = {"original": [original_sentence], "prediction": generated_sentence}
    return grouped_items


def extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items: Dict) -> Tuple[List[List[str]], List[str]]:
    """

    Args:
        grouped_items (Dict): _description_

    Returns:
        Tuple[List[List[str]], List[str]]: tuple of references with:
            - references: List of len as max_num_of_refs, where each element is another list of len as |hypotheses|,
                with either a reference or "" for each hypothesis
            - hypotheses: List of predictions/hypotheses/generated sentences
    """
    max_num_of_refs = max([len(entry["original"]) for entry in grouped_items.values()])
    hypotheses = []
    references = [[] for _ in range(max_num_of_refs)]
    for item in grouped_items.values():
        hypotheses.append(item["prediction"])
        for i in range(max_num_of_refs):
            try:
                references[i].append(item["original"][i])
            except:
                references[i].append("")
    return references, hypotheses


def extract_refs_and_hyps_from_grouped_items(grouped_items: Dict, pad_originals_length: bool = False) -> Tuple[List[List[str]], List[str]]:
    """_summary_

    Args:
        grouped_items (Dict): _description_
        pad_originals_length (bool, optional): _description_. Defaults to False.

    Returns:
        Tuple[List[List[str]], List[str]]: tuple of references with:
            - references: List of len as |hypotheses|, where each element is another list with all the
                references for a given hypotesis. May be padded to make these lists all have the same size
                as the max num of references available for any given hypothesis.
            - hypotheses: List of predictions/hypotheses/generated sentences
    """
    max_num_of_refs = max([len(entry["original"]) for entry in grouped_items.values()])
    hypotheses = []
    references = []
    for item in grouped_items.values():
        hypotheses.append(item["prediction"])
        if pad_originals_length and len(item["original"]) < max_num_of_refs:
            item["original"].extend(["" for _ in range(max_num_of_refs - len(item["original"]))])
        references.append(item["original"])
    return references, hypotheses


def corpus_level_bleu(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[Tuple[str, str, List[str]]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences 

    Returns:
        float: corpus-level bleu 
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    return sacrebleu.corpus_bleu(hypotheses, references).score


def corpus_level_chrf(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[List[str]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences 

    Returns:
        float: corpus-level chrf 
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    chrf = sacrebleu.metrics.CHRF()
    return chrf.corpus_score(hypotheses, references).score


def corpus_level_ter(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[List[str]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences 

    Returns:
        float: corpus-level ter 
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    ter = sacrebleu.metrics.TER()
    return ter.corpus_score(hypotheses, references).score


def corpus_level_meteor(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
    # references = [[nltk.word_tokenize(ref) for ref in ref_group] for ref_group in references]
    # hypotheses = [nltk.word_tokenize(hyp) for hyp in hypotheses]
    scores = []
    for refs, hyp in zip(references, hypotheses):
        scores.append(meteor_score(refs, hyp))
    return sum(scores)*100/len(scores)


def corpus_level_rogue(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
    rouge = PyRouge(rouge_n=False, rouge_l=True)
    scores = rouge.evaluate(hypotheses, references)
    return scores["rouge-l"]["f"]*100


# def corpus_level_mover_score(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
#     grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
#     references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
#     idf_dict_pred = get_idf_dict(hypotheses)
#     idf_dict_og = get_idf_dict(references)
#     scores = word_mover_score(references, hypotheses, idf_dict_og, idf_dict_pred, \
#                             stop_words=[], n_gram=1, remove_subwords=True)
#     return scores
    
